
@article{alghassi_variational_2022,
	title = {A variational quantum algorithm for the {Feynman}-{Kac} formula},
	volume = {6},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2108.10846},
	doi = {10.22331/q-2022-06-07-730},
	abstract = {We propose an algorithm based on variational quantum imaginary time evolution for solving the Feynman-Kac partial differential equation resulting from a multidimensional system of stochastic differential equations. We utilize the correspondence between the Feynman-Kac partial differential equation (PDE) and the Wick-rotated Schr{\textbackslash}"\{o\}dinger equation for this purpose. The results for a \$(2+1)\$ dimensional Feynman-Kac system obtained through the variational quantum algorithm are then compared against classical ODE solvers and Monte Carlo simulation. We see a remarkable agreement between the classical methods and the quantum variational method for an illustrative example on six and eight qubits. In the non-trivial case of PDEs which are preserving probability distributions -- rather than preserving the \${\textbackslash}ell\_2\$-norm -- we introduce a proxy norm which is efficient in keeping the solution approximately normalized throughout the evolution. The algorithmic complexity and costs associated to this methodology, in particular for the extraction of properties of the solution, are investigated. Future research topics in the areas of quantitative finance and other types of PDEs are also discussed.},
	urldate = {2022-12-12},
	journal = {Quantum},
	author = {Alghassi, Hedayat and Deshmukh, Amol and Ibrahim, Noelle and Robles, Nicolas and Woerner, Stefan and Zoufal, Christa},
	month = jun,
	year = {2022},
	note = {arXiv:2108.10846 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {730},
	file = {arXiv Fulltext PDF:files/95/Alghassi 等 - 2022 - A variational quantum algorithm for the Feynman-Ka.pdf:application/pdf;arXiv.org Snapshot:files/96/2108.html:text/html},
}

@article{leong_variational_2022,
	title = {Variational quantum evolution equation solver},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-14906-3},
	doi = {10.1038/s41598-022-14906-3},
	abstract = {Abstract
            Variational quantum algorithms offer a promising new paradigm for solving partial differential equations on near-term quantum computers. Here, we propose a variational quantum algorithm for solving a general evolution equation through implicit time-stepping of the Laplacian operator. The use of encoded source states informed by preceding solution vectors results in faster convergence compared to random re-initialization. Through statevector simulations of the heat equation, we demonstrate how the time complexity of our algorithm scales with the Ansatz volume for gradient estimation and how the time-to-solution scales with the diffusion parameter. Our proposed algorithm extends economically to higher-order time-stepping schemes, such as the Crank–Nicolson method. We present a semi-implicit scheme for solving systems of evolution equations with non-linear terms, such as the reaction–diffusion and the incompressible Navier–Stokes equations, and demonstrate its validity by proof-of-concept results.},
	language = {en},
	number = {1},
	urldate = {2022-12-12},
	journal = {Scientific Reports},
	author = {Leong, Fong Yew and Ewe, Wei-Bin and Koh, Dax Enshan},
	month = jun,
	year = {2022},
	pages = {10817},
}


@article{childs_high-precision_2021,
	title = {High-precision quantum algorithms for partial differential equations},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2002.07868},
	doi = {10.22331/q-2021-11-10-574},
	abstract = {Quantum computers can produce a quantum encoding of the solution of a system of differential equations exponentially faster than a classical algorithm can produce an explicit description. However, while high-precision quantum algorithms for linear ordinary differential equations are well established, the best previous quantum algorithms for linear partial differential equations (PDEs) have complexity \${\textbackslash}mathrm\{poly\}(1/{\textbackslash}epsilon)\$, where \${\textbackslash}epsilon\$ is the error tolerance. By developing quantum algorithms based on adaptive-order finite difference methods and spectral methods, we improve the complexity of quantum algorithms for linear PDEs to be \${\textbackslash}mathrm\{poly\}(d, {\textbackslash}log(1/{\textbackslash}epsilon))\$, where \$d\$ is the spatial dimension. Our algorithms apply high-precision quantum linear system algorithms to systems whose condition numbers and approximation errors we bound. We develop a finite difference algorithm for the Poisson equation and a spectral algorithm for more general second-order elliptic equations.},
	urldate = {2022-10-29},
	journal = {Quantum},
	author = {Childs, Andrew M. and Liu, Jin-Peng and Ostrander, Aaron},
	month = nov,
	year = {2021},
	note = {arXiv:2002.07868 [quant-ph]},
	keywords = {Mathematics - Numerical Analysis, Quantum Physics},
	pages = {574},
	file = {arXiv Fulltext PDF:files/70/Childs 等。 - 2021 - High-precision quantum algorithms for partial diff.pdf:application/pdf;arXiv.org Snapshot:files/71/2002.html:text/html},
}


@misc{patel_quantum-inspired_2022,
	title = {Quantum-{Inspired} {Tensor} {Neural} {Networks} for {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2208.02235},
	abstract = {Partial Differential Equations (PDEs) are used to model a variety of dynamical systems in science and engineering. Recent advances in deep learning have enabled us to solve them in a higher dimension by addressing the curse of dimensionality in new ways. However, deep learning methods are constrained by training time and memory. To tackle these shortcomings, we implement Tensor Neural Networks (TNN), a quantum-inspired neural network architecture that leverages Tensor Network ideas to improve upon deep learning approaches. We demonstrate that TNN provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. We benchmark TNN by applying them to solve parabolic PDEs, specifically the Black-Scholes-Barenblatt equation, widely used in financial pricing theory, empirically showing the advantages of TNN over DNN. Further examples, such as the Hamilton-Jacobi-Bellman equation, are also discussed.},
	urldate = {2022-10-29},
	publisher = {arXiv},
	author = {Patel, Raj and Hsing, Chia-Wei and Sahin, Serkan and Jahromi, Saeed S. and Palmer, Samuel and Sharma, Shivam and Michel, Christophe and Porte, Vincent and Abid, Mustafa and Aubert, Stephane and Castellani, Pierre and Lee, Chi-Guhn and Mugel, Samuel and Orus, Roman},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02235 [cond-mat, physics:physics, physics:quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics, Computer Science - Artificial Intelligence, Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:files/77/Patel 等。 - 2022 - Quantum-Inspired Tensor Neural Networks for Partia.pdf:application/pdf;arXiv.org Snapshot:files/78/2208.html:text/html},
}

@article{yuan_theory_2019,
	title = {Theory of variational quantum simulation},
	volume = {3},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/1812.08767},
	doi = {10.22331/q-2019-10-07-191},
	abstract = {The variational method is a versatile tool for classical simulation of a variety of quantum systems. Great efforts have recently been devoted to its extension to quantum computing for efficiently solving static many-body problems and simulating real and imaginary time dynamics. In this work, we first review the conventional variational principles, including the Rayleigh-Ritz method for solving static problems, and the Dirac and Frenkel variational principle, the McLachlan's variational principle, and the time-dependent variational principle, for simulating real time dynamics. We focus on the simulation of dynamics and discuss the connections of the three variational principles. Previous works mainly focus on the unitary evolution of pure states. In this work, we introduce variational quantum simulation of mixed states under general stochastic evolution. We show how the results can be reduced to the pure state case with a correction term that takes accounts of global phase alignment. For variational simulation of imaginary time evolution, we also extend it to the mixed state scenario and discuss variational Gibbs state preparation. We further elaborate on the design of ansatz that is compatible with post-selection measurement and the implementation of the generalised variational algorithms with quantum circuits. Our work completes the theory of variational quantum simulation of general real and imaginary time evolution and it is applicable to near-term quantum hardware.},
	urldate = {2022-12-12},
	journal = {Quantum},
	author = {Yuan, Xiao and Endo, Suguru and Zhao, Qi and Li, Ying and Benjamin, Simon},
	month = oct,
	year = {2019},
	note = {arXiv:1812.08767 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {191},
	file = {arXiv Fulltext PDF:files/81/Yuan 等 - 2019 - Theory of variational quantum simulation.pdf:application/pdf;arXiv.org Snapshot:files/82/1812.html:text/html},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1718942115},
	doi = {10.1073/pnas.1718942115},
	abstract = {Significance
            Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the “curse of dimensionality.” This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.
          , 
            Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	language = {en},
	number = {34},
	urldate = {2022-12-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	pages = {8505--8510},
	file = {全文:files/84/Han 等 - 2018 - Solving high-dimensional partial differential equa.pdf:application/pdf},
}

@misc{zoufal_error_2021,
	title = {Error {Bounds} for {Variational} {Quantum} {Time} {Evolution}},
	url = {http://arxiv.org/abs/2108.00022},
	abstract = {Variational quantum time evolution (VarQTE) allows us to simulate dynamical quantum systems with parameterized quantum circuits. We derive a posteriori, global phase-agnostic error bounds for real and imaginary time evolution based on McLachlan's variational principle that can be evaluated efficiently. Rigorous error bounds are crucial in practice to adaptively choose variational circuits and to analyze the quality of optimization algorithms. The power of the new error bounds, as well as, the performance of VarQTE are demonstrated on numerical examples.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Zoufal, Christa and Sutter, David and Woerner, Stefan},
	month = jul,
	year = {2021},
	note = {arXiv:2108.00022 [quant-ph]},
	keywords = {Quantum Physics},
	file = {arXiv Fulltext PDF:files/86/Zoufal 等 - 2021 - Error Bounds for Variational Quantum Time Evolutio.pdf:application/pdf;arXiv.org Snapshot:files/87/2108.html:text/html},
}

@book{e_applied_2019,
	address = {Providence, Rhode Island},
	series = {Graduate studies in mathematics},
	title = {Applied stochastic analysis},
	isbn = {978-1-4704-4933-9},
	number = {volume 199},
	publisher = {American Mathematical Society},
	author = {E, Weinan and Li, Tiejun and Vanden-Eijnden, Eric},
	year = {2019},
	keywords = {Numerical analysis -- Probabilistic methods, simulation and stochastic differential equations -- Monte Carlo methods, Numerical analysis -- Probabilistic methods, simulation and stochastic differential equations -- Random number generation, Probability theory and stochastic processes -- Instructional exposition (textbooks, tutorial papers, etc.), Probability theory and stochastic processes -- Markov processes -- Computational methods in Markov chains, Probability theory and stochastic processes -- Stochastic analysis -- Computational methods for stochastic equations, Probability theory and stochastic processes -- Stochastic analysis -- Stochastic ordinary differential equations, Statistical mechanics, structure of matter -- Instructional exposition (textbooks, tutorial papers, etc.), Statistics -- Applications -- Applications to biology and medical sciences, Statistics -- Applications -- Applications to physics, Stochastic analysis},
}

@book{evans_partial_1998,
	address = {Providence, R.I},
	series = {Graduate studies in mathematics},
	title = {Partial differential equations},
	isbn = {978-0-8218-0772-9},
	number = {v. 19},
	publisher = {American Mathematical Society},
	author = {Evans, Lawrence C.},
	year = {1998},
	keywords = {Differential equations, Partial},
}

@article{lubasch_variational_2020,
	title = {Variational quantum algorithms for nonlinear problems},
	volume = {101},
	issn = {2469-9926, 2469-9934},
	url = {http://arxiv.org/abs/1907.09032},
	doi = {10.1103/PhysRevA.101.010301},
	abstract = {We show that nonlinear problems including nonlinear partial differential equations can be efficiently solved by variational quantum computing. We achieve this by utilizing multiple copies of variational quantum states to treat nonlinearities efficiently and by introducing tensor networks as a programming paradigm. The key concepts of the algorithm are demonstrated for the nonlinear Schr{\textbackslash}"\{o\}dinger equation as a canonical example. We numerically show that the variational quantum ansatz can be exponentially more efficient than matrix product states and present experimental proof-of-principle results obtained on an IBM Q device.},
	number = {1},
	urldate = {2022-12-12},
	journal = {Physical Review A},
	author = {Lubasch, Michael and Joo, Jaewoo and Moinier, Pierre and Kiffner, Martin and Jaksch, Dieter},
	month = jan,
	year = {2020},
	note = {arXiv:1907.09032 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {010301},
	file = {arXiv Fulltext PDF:files/92/Lubasch 等 - 2020 - Variational quantum algorithms for nonlinear probl.pdf:application/pdf;arXiv.org Snapshot:files/93/1907.html:text/html},
}

@article{e_machine_2020,
	title = {Machine {Learning} and {Computational} {Mathematics}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	url = {http://arxiv.org/abs/2009.14596},
	doi = {10.4208/cicp.OA-2020-0185},
	abstract = {Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of "black box" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, \{can\} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.},
	number = {5},
	urldate = {2022-12-12},
	journal = {Communications in Computational Physics},
	author = {E, Weinan},
	month = jun,
	year = {2020},
	note = {arXiv:2009.14596 [cs, math, stat]},
	keywords = {68T07, 46E15, 26B35, 26B40, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	pages = {1639--1670},
	file = {arXiv Fulltext PDF:files/100/E - 2020 - Machine Learning and Computational Mathematics.pdf:application/pdf;arXiv.org Snapshot:files/101/2009.html:text/html},
}

@misc{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10561 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:files/103/Raissi 等 - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf;arXiv.org Snapshot:files/104/1711.html:text/html},
}
@article{DBLP:journals/corr/abs-1806-07366,
  author    = {Tian Qi Chen and
               Yulia Rubanova and
               Jesse Bettencourt and
               David Duvenaud},
  title     = {Neural Ordinary Differential Equations},
  journal   = {CoRR},
  volume    = {abs/1806.07366},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.07366},
  eprinttype = {arXiv},
  eprint    = {1806.07366},
  timestamp = {Mon, 22 Jul 2019 14:09:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-07366.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}
